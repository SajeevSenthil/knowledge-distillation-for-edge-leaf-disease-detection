{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bca07ed",
   "metadata": {},
   "source": [
    "# **An efficiency analysis of ConvNeXt-guided distillation for edge-based plant disease detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31f4cd",
   "metadata": {},
   "source": [
    "# Phase 1: Environment Setup and Data Preprocessing\n",
    "---\n",
    "**Framework:** PyTorch | Batch Size: 16 | Input Dimensions: 224x224  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59617c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def dataset_summary(name, ds):\n",
    "    print(f\"\\n{name} Dataset Summary\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    try:\n",
    "        print(f\"Total images: {len(ds)}\")\n",
    "    except TypeError:\n",
    "        print(\"Could not determine dataset length\")\n",
    "        return\n",
    "\n",
    "    # If ImageFolder-style dataset\n",
    "    if hasattr(ds, \"classes\"):\n",
    "        print(f\"Number of classes: {len(ds.classes)}\")\n",
    "        print(\"Classes:\", ds.classes)\n",
    "\n",
    "    # Try to get labels\n",
    "    labels = None\n",
    "    if hasattr(ds, \"targets\"):\n",
    "        labels = ds.targets\n",
    "    elif hasattr(ds, \"samples\"):\n",
    "        labels = [label for _, label in ds.samples]\n",
    "\n",
    "    if labels is not None:\n",
    "        class_counts = Counter(labels)\n",
    "        print(\"\\nImages per class:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            class_name = ds.classes[cls] if hasattr(ds, \"classes\") else cls\n",
    "            print(f\"  {class_name}: {count}\")\n",
    "    else:\n",
    "        print(\"Could not extract class-wise counts\")\n",
    "\n",
    "\n",
    "# ---- Call this for your datasets ----\n",
    "if 'dataset' in globals():\n",
    "    dataset_summary(\"Training\", dataset)\n",
    "\n",
    "if 'val_dataset' in globals():\n",
    "    dataset_summary(\"Validation\", val_dataset)\n",
    "\n",
    "if 'test_dataset' in globals():\n",
    "    dataset_summary(\"Testing\", test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a37510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = \"plant_dataset\"\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "num_classes = len(os.listdir(data_dir))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            p = torch.argmax(out, dim=1)\n",
    "            preds.extend(p.cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "    return (\n",
    "        accuracy_score(labels, preds),\n",
    "        precision_score(labels, preds, average=\"macro\"),\n",
    "        recall_score(labels, preds, average=\"macro\"),\n",
    "        f1_score(labels, preds, average=\"macro\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18694ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.1+cpu\n",
      "Torchvision: 0.24.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0b578",
   "metadata": {},
   "source": [
    "# Phase 2: Dataset Partitioning and Loader Initialization\n",
    "---\n",
    "**Data Splits:** Train, Validation, Test | Batch Size: 16 | Transformation: Resize (224x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbd28c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Healthy', 'Powdery', 'Rust']\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"plant_dataset\"\n",
    "\n",
    "train_dir = os.path.join(data_dir, \"Train\", \"Train\")\n",
    "val_dir = os.path.join(data_dir, \"Validation\", \"Validation\")\n",
    "test_dir = os.path.join(data_dir, \"Test\", \"Test\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fce462",
   "metadata": {},
   "source": [
    "# Phase 3: Model Training and Performance Evaluation\n",
    "---\n",
    "**Architecture:** MobileNetV2 (Pre-trained)  \n",
    "**Optimizer:** AdamW | **Loss Function:** Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f427f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "mobilenet = mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "mobilenet.classifier[1] = nn.Linear(\n",
    "    mobilenet.classifier[1].in_features, num_classes\n",
    ")\n",
    "mobilenet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(mobilenet.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mobilenet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch [{epoch+1}/{epochs}]\",\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = mobilenet(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(mobilenet, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{epochs}] Completed | \"\n",
    "        f\"Avg Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {acc:.4f} | \"\n",
    "        f\"Prec: {prec:.4f} | \"\n",
    "        f\"Rec: {rec:.4f} | \"\n",
    "        f\"F1: {f1:.4f}\"\n",
    "    )\n",
    "\n",
    "acc_m, prec_m, rec_m, f1_m = evaluate(mobilenet, test_loader)\n",
    "\n",
    "print(\"\\nFinal MobileNetV2 Test Results\")\n",
    "print(\"Accuracy:\", acc_m)\n",
    "print(\"Precision:\", prec_m)\n",
    "print(\"Recall:\", rec_m)\n",
    "print(\"F1-score:\", f1_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e126be",
   "metadata": {},
   "source": [
    "# Phase 3.1: Computational Complexity and Efficiency Analysis\n",
    "---\n",
    "**Metric:** FLOPs Calculation | Library: fvcore | Input Tensor: (1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc2166be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::hardtanh_ encountered 35 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 FLOPs: 0.313 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "import torch\n",
    "\n",
    "mobilenet.eval()\n",
    "\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "flops_mobilenet = FlopCountAnalysis(mobilenet, dummy)\n",
    "total_flops_mobilenet = flops_mobilenet.total()\n",
    "\n",
    "print(f\"MobileNetV2 FLOPs: {total_flops_mobilenet / 1e9:.3f} GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783e005",
   "metadata": {},
   "source": [
    "# Phase 3.2: Model Deployment Profiling and Efficiency Metrics\n",
    "---\n",
    "**Metric:** Inference Latency | Resource: Parameters & FLOPs | Storage: Model Size (MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55fcdf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::hardtanh_ encountered 35 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Parameters (M)</th>\n",
       "      <th>FLOPs (GFLOPs)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Accuracy / Million Params</th>\n",
       "      <th>Accuracy / GFLOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MobileNetV2 (Baseline)</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.980644</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.979854</td>\n",
       "      <td>2.227715</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>8.728404</td>\n",
       "      <td>0.439913</td>\n",
       "      <td>3.13182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy  Precision  Recall  F1-Score  \\\n",
       "0  MobileNetV2 (Baseline)      0.98   0.980644    0.98  0.979854   \n",
       "\n",
       "   Parameters (M)  FLOPs (GFLOPs)  Inference Time (s)  Model Size (MB)  \\\n",
       "0        2.227715        0.312917            0.010406         8.728404   \n",
       "\n",
       "   Accuracy / Million Params  Accuracy / GFLOP  \n",
       "0                   0.439913           3.13182  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# ------------------------------\n",
    "# Count TOTAL parameters\n",
    "# ------------------------------\n",
    "def count_all_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Measure inference time\n",
    "# ------------------------------\n",
    "def measure_inference_time(model, device, runs=50):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):  # warm-up\n",
    "            _ = model(dummy_input)\n",
    "        start = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(dummy_input)\n",
    "        end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Parameter count\n",
    "# ------------------------------\n",
    "mobilenet_params = count_all_parameters(mobilenet)\n",
    "\n",
    "# ------------------------------\n",
    "# Inference time\n",
    "# ------------------------------\n",
    "mobilenet_inference_time = measure_inference_time(mobilenet, device)\n",
    "\n",
    "# ------------------------------\n",
    "# Model size\n",
    "# ------------------------------\n",
    "torch.save(mobilenet.state_dict(), \"mobilenetv2.pth\")\n",
    "mobilenet_model_size_mb = os.path.getsize(\"mobilenetv2.pth\") / (1024 * 1024)\n",
    "\n",
    "# ------------------------------\n",
    "# FLOPs using fvcore (GFLOPs)\n",
    "# ------------------------------\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "total_flops_mobilenet = FlopCountAnalysis(mobilenet, dummy).total() / 1e9  # GFLOPs\n",
    "\n",
    "# ------------------------------\n",
    "# Efficiency metrics\n",
    "# ------------------------------\n",
    "accuracy_per_million_params = acc_m / (mobilenet_params / 1e6)\n",
    "accuracy_per_gflop = acc_m / total_flops_mobilenet\n",
    "\n",
    "# ------------------------------\n",
    "# Summary table\n",
    "# ------------------------------\n",
    "mobilenet_summary = pd.DataFrame([{\n",
    "    \"Model\": \"MobileNetV2 (Baseline)\",\n",
    "    \"Accuracy\": acc_m,\n",
    "    \"Precision\": prec_m,\n",
    "    \"Recall\": rec_m,\n",
    "    \"F1-Score\": f1_m,\n",
    "    \"Parameters (M)\": mobilenet_params / 1e6,\n",
    "    \"FLOPs (GFLOPs)\": total_flops_mobilenet,\n",
    "    \"Inference Time (s)\": mobilenet_inference_time,\n",
    "    \"Model Size (MB)\": mobilenet_model_size_mb,\n",
    "    \"Accuracy / Million Params\": accuracy_per_million_params,\n",
    "    \"Accuracy / GFLOP\": accuracy_per_gflop\n",
    "}])\n",
    "\n",
    "# ------------------------------\n",
    "# Save results\n",
    "# ------------------------------\n",
    "mobilenet_summary.to_csv(\"mobilenet_efficiency_metrics.csv\", index=False)\n",
    "\n",
    "mobilenet_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b97b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Train samples:\", len(train_dataset))\\nprint(\"Validation samples:\", len(val_dataset))\\nprint(\"Test samples:\", len(test_dataset))'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666b0781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880649c2",
   "metadata": {},
   "source": [
    "# Phase 4: Advanced ConvNeXtV2 Model Implementation \n",
    "---\n",
    "**Architecture:** ConvNeXt-Tiny (V2-Inspired) | Optimization: AdamW + Cosine Annealing | \n",
    "**Regularization:** GRN & Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4f96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|██████████| 83/83 [04:15<00:00,  3.08s/it, loss=0.323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed | Loss: 0.4283 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|██████████| 83/83 [03:41<00:00,  2.67s/it, loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Completed | Loss: 0.3041 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|██████████| 83/83 [03:54<00:00,  2.82s/it, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Completed | Loss: 0.2983 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|██████████| 83/83 [03:50<00:00,  2.77s/it, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Completed | Loss: 0.2947 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|██████████| 83/83 [03:41<00:00,  2.66s/it, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Completed | Loss: 0.2931 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "\n",
      "ConvNeXt V2-Inspired Teacher Test Results\n",
      "Accuracy: 0.9733333333333334\n",
      "Precision: 0.9741876310272537\n",
      "Recall: 0.9733333333333333\n",
      "F1-score: 0.9733188165920482\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import convnext_tiny\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Global Response Normalization (ConvNeXt V2 idea) ----------\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n",
    "        return self.gamma * (x * nx) + self.beta + x\n",
    "\n",
    "\n",
    "# ---------- ConvNeXt V2–Inspired Teacher Model ----------\n",
    "class ConvNeXtV2Inspired(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = convnext_tiny(weights=\"IMAGENET1K_V1\")\n",
    "        in_features = self.backbone.classifier[2].in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(in_features),\n",
    "            nn.Unflatten(1, (in_features, 1, 1)),\n",
    "            GRN(in_features),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "# ---------- Model, Loss, Optimizer, Scheduler ----------\n",
    "model = ConvNeXtV2Inspired(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=epochs\n",
    ")\n",
    "\n",
    "\n",
    "# ---------- Training Loop ----------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch [{epoch+1}/{epochs}]\",\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} Completed | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {acc:.4f} | \"\n",
    "        f\"Prec: {prec:.4f} | \"\n",
    "        f\"Rec: {rec:.4f} | \"\n",
    "        f\"F1: {f1:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- Final Test Evaluation ----------\n",
    "acc_c, prec_c, rec_c, f1_c = evaluate(model, test_loader)\n",
    "\n",
    "print(\"\\nConvNeXt V2-Inspired Teacher Test Results\")\n",
    "print(\"Accuracy:\", acc_c)\n",
    "print(\"Precision:\", prec_c)\n",
    "print(\"Recall:\", rec_c)\n",
    "print(\"F1-score:\", f1_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40124f24",
   "metadata": {},
   "source": [
    "Save the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72598c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to convnext_v2_teacher.pth\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save Model ----------\n",
    "save_path = \"convnext_v2_teacher.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),  # optional\n",
    "    \"num_classes\": num_classes\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ec7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import convnext_tiny\n",
    "\n",
    "# ---------- Recreate GRN ----------\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + self.eps)\n",
    "        return self.gamma * (x * nx) + self.beta + x\n",
    "\n",
    "\n",
    "# ---------- Recreate Model Architecture ----------\n",
    "class ConvNeXtV2Inspired(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = convnext_tiny(weights=None)  # IMPORTANT\n",
    "        in_features = self.backbone.classifier[2].in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(in_features),\n",
    "            nn.Unflatten(1, (in_features, 1, 1)),\n",
    "            GRN(in_features),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "# ---------- Load Saved Model ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = torch.load(\"convnext_v2_teacher.pth\", map_location=device)\n",
    "\n",
    "num_classes = checkpoint[\"num_classes\"]\n",
    "\n",
    "model = ConvNeXtV2Inspired(num_classes).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()  # VERY IMPORTANT for inference\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4897218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Hybrid KD MobileNetV2\n"
     ]
    }
   ],
   "source": [
    "print(type(teacher))\n",
    "print(teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee0483df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__vsc_ipynb_file__', '_i', '_ii', '_iii', '_i1', 'torch', 'torchvision', '_i2', '_i3', 'optuna', 'nn', 'F', 'mobilenet_v2', 'tqdm', 'DistillationLoss', 'objective', 'study', '_i4', 'os', 'optim', 'datasets', 'transforms', 'DataLoader', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score', 'device', 'data_dir', 'batch_size', 'epochs', 'num_classes', 'transform', 'dataset', 'loader', 'evaluate', '_i5', 'train_dir', 'val_dir', 'test_dir', 'train_dataset', 'val_dataset', 'test_dataset', 'train_loader', 'val_loader', 'test_loader', '_i6', '_i7', '_i8', '_i9', '_i10', '_i11', '_i12', '_i13', 'teacher', '_i14', '_i15', '_i16', '_i17', 'convnext_tiny', 'GRN', 'ConvNeXtV2Inspired', 'model', 'criterion', 'optimizer', 'scheduler', 'epoch', 'running_loss', 'progress_bar', 'x', 'y', 'out', 'loss', 'avg_loss', 'acc', 'prec', 'rec', 'f1', 'acc_c', 'prec_c', 'rec_c', 'f1_c', '_i18', 'tpe_objective', '_i19', '_i20', '_i21'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68c848",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da7d933",
   "metadata": {},
   "source": [
    "# Phase 4.2: Teacher Model Profiling and Efficiency Metrics\n",
    "---\n",
    "**Metrics:** Parametric Density | Performance: Inference Latency | Storage: Binary Model Size (MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aabe9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 18 time(s)\n",
      "Unsupported operator aten::mul encountered 20 time(s)\n",
      "Unsupported operator aten::add_ encountered 18 time(s)\n",
      "Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "Unsupported operator aten::linalg_vector_norm encountered 1 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 3 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.features.1.0.stochastic_depth, backbone.features.1.1.stochastic_depth, backbone.features.1.2.stochastic_depth, backbone.features.3.0.stochastic_depth, backbone.features.3.1.stochastic_depth, backbone.features.3.2.stochastic_depth, backbone.features.5.0.stochastic_depth, backbone.features.5.1.stochastic_depth, backbone.features.5.2.stochastic_depth, backbone.features.5.3.stochastic_depth, backbone.features.5.4.stochastic_depth, backbone.features.5.5.stochastic_depth, backbone.features.5.6.stochastic_depth, backbone.features.5.7.stochastic_depth, backbone.features.5.8.stochastic_depth, backbone.features.7.0.stochastic_depth, backbone.features.7.1.stochastic_depth, backbone.features.7.2.stochastic_depth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Parameters (M)</th>\n",
       "      <th>FLOPs (GFLOPs)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Accuracy / Million Params</th>\n",
       "      <th>Accuracy / GFLOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConvNeXt V2-Inspired Teacher</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973319</td>\n",
       "      <td>27.823971</td>\n",
       "      <td>4.469672</td>\n",
       "      <td>0.036494</td>\n",
       "      <td>106.207631</td>\n",
       "      <td>0.034982</td>\n",
       "      <td>0.217764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy  Precision    Recall  F1-Score  \\\n",
       "0  ConvNeXt V2-Inspired Teacher  0.973333   0.974188  0.973333  0.973319   \n",
       "\n",
       "   Parameters (M)  FLOPs (GFLOPs)  Inference Time (s)  Model Size (MB)  \\\n",
       "0       27.823971        4.469672            0.036494       106.207631   \n",
       "\n",
       "   Accuracy / Million Params  Accuracy / GFLOP  \n",
       "0                   0.034982          0.217764  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# ------------------------------\n",
    "# Parameter counting (TOTAL params)\n",
    "# ------------------------------\n",
    "def count_all_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Inference time measurement\n",
    "# ------------------------------\n",
    "def measure_inference_time(model, device, runs=50):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):  # warm-up\n",
    "            _ = model(dummy_input)\n",
    "        start = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(dummy_input)\n",
    "        end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TOTAL parameter count\n",
    "# ------------------------------\n",
    "convnext_params = count_all_parameters(model)\n",
    "\n",
    "# ------------------------------\n",
    "# Inference time\n",
    "# ------------------------------\n",
    "convnext_inference_time = measure_inference_time(model, device)\n",
    "\n",
    "# ------------------------------\n",
    "# Model size on disk\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"convnext_v2_inspired.pth\")\n",
    "convnext_model_size_mb = os.path.getsize(\"convnext_v2_inspired.pth\") / (1024 * 1024)\n",
    "\n",
    "# ------------------------------\n",
    "# FLOPs using fvcore (CORRECT METHOD)\n",
    "# ------------------------------\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "total_flops_convnext = FlopCountAnalysis(model, dummy).total() / 1e9  # GFLOPs\n",
    "\n",
    "# ------------------------------\n",
    "# Efficiency metrics\n",
    "# ------------------------------\n",
    "accuracy_per_million_params = acc_c / (convnext_params / 1e6)\n",
    "accuracy_per_gflop = acc_c / total_flops_convnext\n",
    "\n",
    "# ------------------------------\n",
    "# Summary table\n",
    "# ------------------------------\n",
    "convnext_summary = pd.DataFrame([{\n",
    "    \"Model\": \"ConvNeXt V2-Inspired Teacher\",\n",
    "    \"Accuracy\": acc_c,\n",
    "    \"Precision\": prec_c,\n",
    "    \"Recall\": rec_c,\n",
    "    \"F1-Score\": f1_c,\n",
    "    \"Parameters (M)\": convnext_params / 1e6,\n",
    "    \"FLOPs (GFLOPs)\": total_flops_convnext,\n",
    "    \"Inference Time (s)\": convnext_inference_time,\n",
    "    \"Model Size (MB)\": convnext_model_size_mb,\n",
    "    \"Accuracy / Million Params\": accuracy_per_million_params,\n",
    "    \"Accuracy / GFLOP\": accuracy_per_gflop\n",
    "}])\n",
    "\n",
    "# ------------------------------\n",
    "# Save results\n",
    "# ------------------------------\n",
    "convnext_summary.to_csv(\"convnext_v2_inspired_efficiency_metrics.csv\", index=False)\n",
    "\n",
    "convnext_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133c3b6",
   "metadata": {},
   "source": [
    "# Phase 5:Phase 5: Knowledge Distillation (KD) Training Strategy\n",
    "---\n",
    "**Teacher:** ConvNeXt-Tiny (V2-Inspired) | Student: MobileNetV2 | KD Parameters: $\\alpha=0.7$, $T=4.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2ff85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch [1/5]: 100%|██████████| 83/83 [02:51<00:00,  2.06s/it, loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 1 Completed | Loss: 0.2486 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch [2/5]: 100%|██████████| 83/83 [02:49<00:00,  2.04s/it, loss=0.0718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 2 Completed | Loss: 0.1130 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch [3/5]: 100%|██████████| 83/83 [02:54<00:00,  2.10s/it, loss=0.263] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 3 Completed | Loss: 0.1153 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch [4/5]: 100%|██████████| 83/83 [03:15<00:00,  2.35s/it, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 4 Completed | Loss: 0.0970 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KD Epoch [5/5]: 100%|██████████| 83/83 [02:59<00:00,  2.16s/it, loss=0.0522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Epoch 5 Completed | Loss: 0.0805 | Val Acc: 1.0000 | Prec: 1.0000 | Rec: 1.0000 | F1: 1.0000\n",
      "\n",
      "Hybrid Distilled MobileNetV2 Test Results\n",
      "Accuracy: 0.9733333333333334\n",
      "Precision: 0.9741876310272537\n",
      "Recall: 0.9733333333333333\n",
      "F1-score: 0.9733188165920482\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# Knowledge Distillation Loss\n",
    "# ------------------------------\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, temperature=4.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        hard_loss = self.ce(student_logits, labels)\n",
    "\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "\n",
    "        kd_loss = F.kl_div(\n",
    "            soft_student,\n",
    "            soft_teacher,\n",
    "            reduction=\"batchmean\"\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        return self.alpha * hard_loss + (1 - self.alpha) * kd_loss\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Student Model (MobileNetV2)\n",
    "# ------------------------------\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "student = mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "student.classifier[1] = nn.Linear(\n",
    "    student.classifier[1].in_features, num_classes\n",
    ")\n",
    "student.to(device)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Teacher Model (Frozen)\n",
    "# ------------------------------\n",
    "teacher = model   # your ConvNeXt V2–Inspired teacher\n",
    "teacher.eval()\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Optimizer & Loss\n",
    "# ------------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = DistillationLoss(\n",
    "    alpha=0.7,\n",
    "    temperature=4.0\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Distillation Training Loop\n",
    "# ------------------------------\n",
    "epochs_kd = 5\n",
    "\n",
    "for epoch in range(epochs_kd):\n",
    "    student.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"KD Epoch [{epoch+1}/{epochs_kd}]\",\n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(x)\n",
    "\n",
    "        student_logits = student(x)\n",
    "\n",
    "        loss = criterion(student_logits, teacher_logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    acc, prec, rec, f1 = evaluate(student, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"KD Epoch {epoch+1} Completed | \"\n",
    "        f\"Loss: {avg_loss:.4f} | \"\n",
    "        f\"Val Acc: {acc:.4f} | \"\n",
    "        f\"Prec: {prec:.4f} | \"\n",
    "        f\"Rec: {rec:.4f} | \"\n",
    "        f\"F1: {f1:.4f}\"\n",
    "    )\n",
    "  \n",
    "\n",
    "# ------------------------------\n",
    "# Final Test Evaluation\n",
    "# ------------------------------\n",
    "acc_kd, prec_kd, rec_kd, f1_kd = evaluate(student, test_loader)\n",
    "\n",
    "print(\"\\nHybrid Distilled MobileNetV2 Test Results\")\n",
    "print(\"Accuracy:\", acc_kd)\n",
    "print(\"Precision:\", prec_kd)\n",
    "print(\"Recall:\", rec_kd)\n",
    "print(\"F1-score:\", f1_kd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9104bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled model saved to mobilenetv2_distilled.pth\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Save Distilled Student Model\n",
    "# ------------------------------\n",
    "\n",
    "save_path = \"mobilenetv2_distilled.pth\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": student.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),  # optional (for resuming training)\n",
    "    \"num_classes\": num_classes\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Distilled model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3231c14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled MobileNetV2 loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# ------------------------------\n",
    "# Device\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load Checkpoint\n",
    "# ------------------------------\n",
    "checkpoint = torch.load(\"mobilenetv2_distilled.pth\", map_location=device)\n",
    "num_classes = checkpoint[\"num_classes\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Recreate Student Architecture\n",
    "# ------------------------------\n",
    "student = mobilenet_v2(weights=None)  # IMPORTANT: do NOT load ImageNet weights\n",
    "student.classifier[1] = nn.Linear(\n",
    "    student.classifier[1].in_features,\n",
    "    num_classes\n",
    ")\n",
    "\n",
    "student.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "student.to(device)\n",
    "student.eval()  # IMPORTANT for inference\n",
    "\n",
    "print(\"Distilled MobileNetV2 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student.state_dict(), \"hybrid_kd_mobilenetv2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5d62a",
   "metadata": {},
   "source": [
    "# Phase 5.1: Final Model Profiling and Efficiency Analysis (Knowledge Distillation)\n",
    "**Architecture:** Hybrid KD MobileNetV2 | \n",
    "**Metrics:** Parameter Density & GFLOPs | \n",
    "**Efficiency:** Accuracy per Compute Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0402e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::hardtanh_ encountered 35 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Parameters (M)</th>\n",
       "      <th>FLOPs (GFLOPs)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Accuracy / Million Params</th>\n",
       "      <th>Accuracy / GFLOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hybrid KD MobileNetV2 (Final)</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973319</td>\n",
       "      <td>2.227715</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>0.01137</td>\n",
       "      <td>8.731517</td>\n",
       "      <td>0.43692</td>\n",
       "      <td>3.110515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Accuracy  Precision    Recall  F1-Score  \\\n",
       "0  Hybrid KD MobileNetV2 (Final)  0.973333   0.974188  0.973333  0.973319   \n",
       "\n",
       "   Parameters (M)  FLOPs (GFLOPs)  Inference Time (s)  Model Size (MB)  \\\n",
       "0        2.227715        0.312917             0.01137         8.731517   \n",
       "\n",
       "   Accuracy / Million Params  Accuracy / GFLOP  \n",
       "0                    0.43692          3.110515  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def measure_inference_time(model, device, runs=50):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "        start = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(dummy_input)\n",
    "        end = time.time()\n",
    "    return (end - start) / runs\n",
    "\n",
    "\n",
    "# ---------- Parameter Count ----------\n",
    "hybrid_params = count_parameters(student)\n",
    "\n",
    "# ---------- Inference Time ----------\n",
    "hybrid_inference_time = measure_inference_time(student, device)\n",
    "\n",
    "# ---------- Model Size ----------\n",
    "torch.save(student.state_dict(), \"hybrid_kd_mobilenetv2.pth\")\n",
    "hybrid_model_size_mb = os.path.getsize(\"hybrid_kd_mobilenetv2.pth\") / (1024 * 1024)\n",
    "\n",
    "# ---------- FLOPs ----------\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "hybrid_flops = FlopCountAnalysis(student, dummy).total() / 1e9  # GFLOPs\n",
    "\n",
    "# ---------- Efficiency Metric ----------\n",
    "accuracy_per_million_params = acc_kd / (hybrid_params / 1e6)\n",
    "accuracy_per_gflop = acc_kd / hybrid_flops\n",
    "\n",
    "\n",
    "# ---------- Summary Table ----------\n",
    "hybrid_summary = pd.DataFrame([{\n",
    "    \"Model\": \"Hybrid KD MobileNetV2 (Final)\",\n",
    "    \"Accuracy\": acc_kd,\n",
    "    \"Precision\": prec_kd,\n",
    "    \"Recall\": rec_kd,\n",
    "    \"F1-Score\": f1_kd,\n",
    "    \"Parameters (M)\": hybrid_params / 1e6,\n",
    "    \"FLOPs (GFLOPs)\": hybrid_flops,\n",
    "    \"Inference Time (s)\": hybrid_inference_time,\n",
    "    \"Model Size (MB)\": hybrid_model_size_mb,\n",
    "    \"Accuracy / Million Params\": accuracy_per_million_params,\n",
    "    \"Accuracy / GFLOP\": accuracy_per_gflop\n",
    "}])\n",
    "\n",
    "hybrid_summary.to_csv(\"hybrid_model_efficiency_metrics.csv\", index=False)\n",
    "\n",
    "hybrid_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c75db",
   "metadata": {},
   "source": [
    "# Phase 6: Comprehensive Comparative Analysis and Model Benchmarking\n",
    "---\n",
    "**Evaluation Scope:** Baseline vs. Teacher vs. Hybrid Student|\n",
    "**Hardware:** CUDA/CPU Synchronized Timing| \n",
    "**Metrics:** Accuracy/Complexity Trade-off|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ced335b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::hardtanh_ encountered 35 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n",
      "Unsupported operator aten::gelu encountered 18 time(s)\n",
      "Unsupported operator aten::mul encountered 20 time(s)\n",
      "Unsupported operator aten::add_ encountered 18 time(s)\n",
      "Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "Unsupported operator aten::linalg_vector_norm encountered 1 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 3 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.features.1.0.stochastic_depth, backbone.features.1.1.stochastic_depth, backbone.features.1.2.stochastic_depth, backbone.features.3.0.stochastic_depth, backbone.features.3.1.stochastic_depth, backbone.features.3.2.stochastic_depth, backbone.features.5.0.stochastic_depth, backbone.features.5.1.stochastic_depth, backbone.features.5.2.stochastic_depth, backbone.features.5.3.stochastic_depth, backbone.features.5.4.stochastic_depth, backbone.features.5.5.stochastic_depth, backbone.features.5.6.stochastic_depth, backbone.features.5.7.stochastic_depth, backbone.features.5.8.stochastic_depth, backbone.features.7.0.stochastic_depth, backbone.features.7.1.stochastic_depth, backbone.features.7.2.stochastic_depth\n",
      "Unsupported operator aten::hardtanh_ encountered 35 time(s)\n",
      "Unsupported operator aten::add encountered 10 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Parameters (M)</th>\n",
       "      <th>FLOPs (GFLOPs)</th>\n",
       "      <th>Inference Time (s)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Accuracy / GFLOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MobileNetV2 (Baseline)</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.980644</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979854</td>\n",
       "      <td>2.227715</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>8.728404</td>\n",
       "      <td>3.131820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ConvNeXt V2-Inspired (Teacher)</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.973319</td>\n",
       "      <td>27.823971</td>\n",
       "      <td>4.469672</td>\n",
       "      <td>0.036628</td>\n",
       "      <td>106.206906</td>\n",
       "      <td>0.217764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid KD MobileNetV2 (Final)</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.980644</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979854</td>\n",
       "      <td>2.227715</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>8.729320</td>\n",
       "      <td>3.131820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  Accuracy  Precision    Recall  F1-Score  \\\n",
       "0          MobileNetV2 (Baseline)  0.980000   0.980644  0.980000  0.979854   \n",
       "1  ConvNeXt V2-Inspired (Teacher)  0.973333   0.974188  0.973333  0.973319   \n",
       "2   Hybrid KD MobileNetV2 (Final)  0.980000   0.980644  0.980000  0.979854   \n",
       "\n",
       "   Parameters (M)  FLOPs (GFLOPs)  Inference Time (s)  Model Size (MB)  \\\n",
       "0        2.227715        0.312917            0.010507         8.728404   \n",
       "1       27.823971        4.469672            0.036628       106.206906   \n",
       "2        2.227715        0.312917            0.009694         8.729320   \n",
       "\n",
       "   Accuracy / GFLOP  \n",
       "0          3.131820  \n",
       "1          0.217764  \n",
       "2          3.131820  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# ------------------------------\n",
    "# Utility: count TOTAL parameters\n",
    "# ------------------------------\n",
    "def count_all_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Utility: CPU/GPU safe inference timing\n",
    "# ------------------------------\n",
    "def measure_inference_time(model, device, runs=30):\n",
    "    model.eval()\n",
    "    dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # warm-up\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start.record()\n",
    "            for _ in range(runs):\n",
    "                _ = model(dummy)\n",
    "            end.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            return start.elapsed_time(end) / (runs * 1000)  # seconds\n",
    "\n",
    "        else:\n",
    "            start = time.time()\n",
    "            for _ in range(runs):\n",
    "                _ = model(dummy)\n",
    "            end = time.time()\n",
    "            return (end - start) / runs\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Dummy input for FLOPs\n",
    "# ------------------------------\n",
    "dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# ==============================\n",
    "# MobileNetV2 (Baseline)\n",
    "# ==============================\n",
    "mobilenet_params = count_all_parameters(mobilenet)\n",
    "mobilenet_flops = FlopCountAnalysis(mobilenet, dummy).total() / 1e9  # GFLOPs\n",
    "mobilenet_time = measure_inference_time(mobilenet, device)\n",
    "torch.save(mobilenet.state_dict(), \"mobilenetv2.pth\")\n",
    "mobilenet_size = os.path.getsize(\"mobilenetv2.pth\") / (1024 * 1024)\n",
    "\n",
    "# ==============================\n",
    "# ConvNeXt V2-Inspired Teacher\n",
    "# ==============================\n",
    "convnext_params = count_all_parameters(model)\n",
    "convnext_flops = FlopCountAnalysis(model, dummy).total() / 1e9  # GFLOPs\n",
    "convnext_time = measure_inference_time(model, device)\n",
    "torch.save(model.state_dict(), \"convnext_teacher.pth\")\n",
    "convnext_size = os.path.getsize(\"convnext_teacher.pth\") / (1024 * 1024)\n",
    "\n",
    "# ==============================\n",
    "# Hybrid KD MobileNetV2 (Student)\n",
    "# ==============================\n",
    "hybrid_params = count_all_parameters(student)\n",
    "hybrid_flops = FlopCountAnalysis(student, dummy).total() / 1e9  # GFLOPs\n",
    "hybrid_time = measure_inference_time(student, device)\n",
    "torch.save(student.state_dict(), \"hybrid_student.pth\")\n",
    "hybrid_size = os.path.getsize(\"hybrid_student.pth\") / (1024 * 1024)\n",
    "\n",
    "# ==============================\n",
    "# Final comparison table\n",
    "# ==============================\n",
    "comparison_table = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"MobileNetV2 (Baseline)\",\n",
    "        \"Accuracy\": acc_m,\n",
    "        \"Precision\": prec_m,\n",
    "        \"Recall\": rec_m,\n",
    "        \"F1-Score\": f1_m,\n",
    "        \"Parameters (M)\": mobilenet_params / 1e6,\n",
    "        \"FLOPs (GFLOPs)\": mobilenet_flops,\n",
    "        \"Inference Time (s)\": mobilenet_time,\n",
    "        \"Model Size (MB)\": mobilenet_size,\n",
    "        \"Accuracy / GFLOP\": acc_m / mobilenet_flops\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"ConvNeXt V2-Inspired (Teacher)\",\n",
    "        \"Accuracy\": acc_c,\n",
    "        \"Precision\": prec_c,\n",
    "        \"Recall\": rec_c,\n",
    "        \"F1-Score\": f1_c,\n",
    "        \"Parameters (M)\": convnext_params / 1e6,\n",
    "        \"FLOPs (GFLOPs)\": convnext_flops,\n",
    "        \"Inference Time (s)\": convnext_time,\n",
    "        \"Model Size (MB)\": convnext_size,\n",
    "        \"Accuracy / GFLOP\": acc_c / convnext_flops\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Hybrid KD MobileNetV2 (Final)\",\n",
    "        \"Accuracy\": acc_kd,\n",
    "        \"Precision\": prec_kd,\n",
    "        \"Recall\": rec_kd,\n",
    "        \"F1-Score\": f1_kd,\n",
    "        \"Parameters (M)\": hybrid_params / 1e6,\n",
    "        \"FLOPs (GFLOPs)\": hybrid_flops,\n",
    "        \"Inference Time (s)\": hybrid_time,\n",
    "        \"Model Size (MB)\": hybrid_size,\n",
    "        \"Accuracy / GFLOP\": acc_kd / hybrid_flops\n",
    "    }\n",
    "])\n",
    "\n",
    "comparison_table.to_csv(\"final_3_model_comparison.csv\", index=False)\n",
    "comparison_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4766f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a1369b",
   "metadata": {},
   "source": [
    "# Trying Olama Bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a62bea",
   "metadata": {},
   "source": [
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba6fa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe92500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038103a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6728af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e160e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Healthy', 'Powdery', 'Rust']\n",
      "Number of classes: 3\n",
      "Checkpoint classes: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"plant_dataset/Train/Train\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=\"plant_dataset/Test/Test\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of classes:\", len(class_names))\n",
    "print(\"Checkpoint classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6ca650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564cfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_image(image_tensor):\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = student(image_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probs, 1)\n",
    "\n",
    "    disease_name = class_names[predicted.item()]\n",
    "    return disease_name, confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2137916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Disease: Healthy\n",
      "Confidence: 0.9522280097007751\n",
      "True Label: Healthy\n"
     ]
    }
   ],
   "source": [
    "image_tensor, true_label = next(iter(test_loader))\n",
    "\n",
    "disease, confidence = predict_image(image_tensor)\n",
    "\n",
    "print(\"Predicted Disease:\", disease)\n",
    "print(\"Confidence:\", confidence)\n",
    "print(\"True Label:\", class_names[true_label.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e280fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_severity_hint(confidence):\n",
    "    if confidence > 0.90:\n",
    "        return \"High confidence prediction. Likely clear visible symptoms.\"\n",
    "    elif confidence > 0.75:\n",
    "        return \"Moderate confidence prediction. Symptoms may be developing.\"\n",
    "    else:\n",
    "        return \"Low confidence prediction. Consider rechecking the plant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aaa1782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AI Agricultural Report ---\n",
      "\n",
      " Confirmation: Your plant appears healthy with a high model confidence of 0.95. Continue providing it with adequate sunlight, water, and nutrients as needed.\n",
      "\n",
      "Preventive Care Steps: Ensure proper watering schedule (avoid overwatering), provide appropriate lighting, and fertilize when necessary using a balanced fertilizer.\n",
      "\n",
      "Monitoring Recommendations: Keep an eye on the plant for any signs of pests or disease. Check the overall health regularly by examining leaves, stems, and roots.\n",
      "\n",
      "Recheck: Schedule a recheck after two weeks to assess if there are any changes in the plant's condition. If issues arise before then, do not hesitate to examine the plant again.\n"
     ]
    }
   ],
   "source": [
    "severity_hint = get_severity_hint(confidence)\n",
    "\n",
    "report = generate_plant_report(disease, confidence, severity_hint)\n",
    "\n",
    "print(\"\\n--- AI Agricultural Report ---\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60bdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama_helper import generate_plant_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae22a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_image(image_tensor):\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = student(image_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probs, 1)\n",
    "\n",
    "    disease_name = class_names[predicted.item()]\n",
    "    \n",
    "    return disease_name, confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6898f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing image: plant_dataset/Test/Test\\Powdery\\80bc7d353e163e85.jpg\n",
      "Predicted: Powdery\n",
      "Confidence: 0.9335793256759644\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Pick class folder manually\n",
    "target_class = \"Powdery\"\n",
    "\n",
    "test_path = \"plant_dataset/Test/Test\"\n",
    "\n",
    "# Get first image from Powdery folder\n",
    "image_file = os.listdir(os.path.join(test_path, target_class))[0]\n",
    "\n",
    "image_path = os.path.join(test_path, target_class, image_file)\n",
    "\n",
    "print(\"Testing image:\", image_path)\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "disease, confidence = predict_image(image_tensor)\n",
    "\n",
    "print(\"Predicted:\", disease)\n",
    "print(\"Confidence:\", confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a48aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AI Agricultural Report ---\n",
      "\n",
      " Title: Comprehensive Plant Care Guide for Farmers\n",
      "\n",
      "1. Preventive Care Tips:\n",
      "   - Regularly inspect your plants for any signs of disease or pests. Early detection can prevent the spread and minimize damage.\n",
      "   - Prune dead, diseased, or damaged parts of the plant to promote growth and improve overall health.\n",
      "   - Rotate crops periodically to reduce the build-up of soil-borne diseases and pests.\n",
      "\n",
      "2. Best Irrigation Practices:\n",
      "   - Water at the base of the plant to avoid moisture build-up on leaves, reducing risk of fungal diseases.\n",
      "   - Use drip irrigation or soaker hoses for efficient water use and consistent moisture levels.\n",
      "   - Avoid overwatering as it can lead to root rot and other diseases.\n",
      "   - Consider using rainwater harvesting systems for sustainable irrigation practices.\n",
      "\n",
      "3. Fertilizer Recommendations:\n",
      "   - Use a balanced fertilizer with equal amounts of nitrogen, phosphorus, and potassium (N-P-K ratio: 10-10-10).\n",
      "   - Test your soil regularly to determine specific nutrient needs.\n",
      "   - Organic matter like compost can improve soil fertility and reduce the need for synthetic fertilizers.\n",
      "   - Fertilize during growth periods, avoiding application just before harvest to prevent residual chemicals on produce.\n",
      "\n",
      "4. Pest Prevention Advice:\n",
      "   - Use beneficial insects such as ladybugs, lacewings, and spiders to control pest populations naturally.\n",
      "   - Encourage biodiversity in your farm by planting a variety of crops, which can attract natural predators of common pests.\n",
      "   - Rotate crop varieties every few years to disrupt the life cycles of pests and diseases.\n",
      "   - Monitor your crops regularly for signs of infestation and address issues promptly with appropriate treatments.\n",
      "\n",
      "5. Seasonal Care Guidance:\n",
      "   - In Spring, prepare soil by adding compost, plant cool-season crops like lettuce and spinach, and watch for early signs of pests.\n",
      "   - During Summer, ensure consistent watering and weeding to prevent stress on plants and competition for resources. Monitor for heat-tolerant pests like aphids or whiteflies.\n",
      "   - In Autumn, plant root vegetables like carrots and beets. Keep an eye out for fall diseases such as powdery mildew.\n",
      "   - During Winter, protect crops from frost damage by using row covers, cold frames, or cloches. Monitor for overwintering pests like cabbage worms or cutworms.\n"
     ]
    }
   ],
   "source": [
    "severity_hint = get_severity_hint(confidence)\n",
    "\n",
    "report = generate_plant_report(disease, confidence, severity_hint)\n",
    "\n",
    "print(\"\\n--- AI Agricultural Report ---\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565a0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
